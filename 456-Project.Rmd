---
title: "Customer Personality Analysis"
author: "Flora He"
date: "`r Sys.Date()`"
output: 
  pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r load-libraries, message=FALSE, warning=FALSE}
# load libraries
library(tidyverse)
library(ggplot2)
library(corrplot)
library(knitr)
library(gridExtra)
library(gridGraphics)
library(randomForest)
library(cluster)
library(mclust)
library(fastDummies)
library(factoextra)
```

```{r}
# Load Data
data = read.csv("marketing_campaign.csv", sep = "\t")
```

```{r}
# DATA CLEANING

# Remove rows with NA values (verify count before/after if needed)
data = na.omit(data)

# Compute Enrollment variable 
# Fix: Ensure date format matches actual data (e.g., "%d-%m-%Y" for "31-12-2023")
data$Dt_Customer = as.Date(data$Dt_Customer, format = "%d-%m-%Y")  # Use correct format
data$Enrollment = as.numeric(as.Date("2023-12-31") - data$Dt_Customer)  # Fixed reference date

# Compute variables
data = data %>%
  mutate(
    Age = 2023 - Year_Birth,  # Adjusted to realistic year
    
    Total_Spending = MntWines + MntFruits + MntMeatProducts + 
      MntFishProducts + MntSweetProducts + MntGoldProds,
    
    Total_Purchases = NumDealsPurchases + NumWebPurchases + 
      NumCatalogPurchases + NumStorePurchases,
    
    Living_Status = case_when(
      Marital_Status %in% c("Married", "Together") ~ "Partner",
      Marital_Status %in% c("Single", "Divorced", "Widow", "Alone", "Absurd", "YOLO") ~ "Alone" 
    ),
    
    Family_Size = 1 + (Living_Status == "Partner") + Kidhome + Teenhome,
    
    Accepted = AcceptedCmp1 + AcceptedCmp2 + AcceptedCmp3 + 
      AcceptedCmp4 + AcceptedCmp5 + Response
  )

# Remove unwanted variables
data = data %>%
  select(-Year_Birth, -Z_CostContact, -Z_Revenue, -Marital_Status, 
         -Dt_Customer, -AcceptedCmp1, -AcceptedCmp2, -AcceptedCmp3, 
         -AcceptedCmp4, -AcceptedCmp5, -Kidhome, -Teenhome)
```

# Summary
This project aims to understand customer behavior through multivariate analysis of demographic, spending, and campaign engagement data. Key objectives include exploring the intrinsic dimensionality of the data, identifying key drivers of purchasing behavior, exploring purchase channel habits, and assessing customer clustering. Using multivariate techniques, we found that the variance in the dataset cannot be captured by a small number of linear combinations of the original variables. Additionally, income, enrollment time, offer acceptance rate, and family size are the most influential predictors of customer spending and purchases. Catalog purchases drive the highest spending, while store purchases drive purchase frequency. Clustering attempts revealed no natural customer segments, suggesting traits like income should be analyzed as continuous variables rather than discrete groups. These insights can guide targeted marketing strategies. \


# Introduction
**Background**

Understanding customer behavior is crucial for targeted marketing, and analyzing dimensionality and predictors may help refine marketing strategies. This dataset includes 2,240 customers with 29 attributes on customer information, product information, promotion information, and purchase methods. It is provided by Dr. Omar Romero-Hernandez, who is a lecturer at the UC-Berkeley Haas School of Business. \

**Problem Statement**

1. What is the intrinsic dimensionality of the data? Can the variance in the dataset be captured by a small number of linear combinations of the original variables? 

2. Which demographic/personality traits predict purchasing behavior? 

3. Do purchase channels (web, catalog, store) influence spending habits? 

4. Can customers be segmented into meaningful groups? \

# Methods & Results
**PCA**

First, we conduct PCA (Principal Component Analysis) to reduce data dimension and explore the dimensionality of the data. In other words, we will explore whether the variance in this dataset can be explained by a few combinations of the variables. PCA was chosen to explore the intrinsic dimensionality here because this method reduces high-dimensional data into fewer components while retaining maximal variance. This makes it ideal for identifying underlying patterns and simplifying complex datasets. We start with one-hot encoding to turn categorical variables into numeric ones and then scale the data. After these steps, we can conduct PCA: \

```{r, fig.width=6, fig.height=4}
# one-hot encoding for categorical variables
categorical_vars = names(data)[sapply(data, function(x) is.factor(x) || is.character(x))]

data_encoded = dummy_cols(data, select_columns = categorical_vars,
                          remove_selected_columns = TRUE)

# scale data
scaled_data = scale(data_encoded)

# PCA
data_pcacor = prcomp(scaled_data)

plot(data_pcacor$sdev^2, 
     xlab = "Principal Component", 
     ylab = "Variance",
     main = "Scree Plot",
     type = "b",  
     pch = 19)


axis(1, at = 1:ncol(scaled_data),  
  labels = 1:ncol(scaled_data)
)
```

```{r}
sdev = data_pcacor$sdev
var_explained = sdev^2
prop_var = var_explained / sum(var_explained)
cum_var = cumsum(prop_var)

pca_summary_tbl = rbind(
  `Standard Deviation` = sdev,
  `Proportion of Variance` = prop_var,
  `Cumulative Proportion` = cum_var
)

pca_summary_tbl_trimmed = pca_summary_tbl[, 1:10]
colnames(pca_summary_tbl_trimmed) = paste0("PC", 1:ncol(pca_summary_tbl_trimmed))

kable(pca_summary_tbl_trimmed, caption = "PCA: Feature Reduction", digits = 3)
```

From the PCA summary table, we can see that we need at least 9 PCs to explain 70% of the variance in data. This aligns with the scree plot since it doesn't show a useful "elbow" that can point to a obvious separate point that could account for the majority of variance in the data. These results together indicate that the variance in this dataset cannot be captured by a small number of linear combinations of the original variables. The high intrinsic dimensionality in this dataset suggests that it is complex and cannot be simplified into a small set of linear components.\




**Random Forest**

Having established the datasetâ€™s complexity, we next identify key predictors using Random Forest. We will do feature selection to determine which attributes are influential to customer behaviors. We will apply Random Forest, which is a relatively stable classifier that utilizes bootstrapping on the data. It allows us to do feature selection and to identify key predictors of customer purchasing behavior. Moreover, since Random Forest can work directly with categorical data, we can use the original cleaned data (without one-hot encoding for categorical features) to proceed. We will train Random Forest models to predict Total Purchases and Total Spending, and then analyze feature importance using %IncMSE (Percent Increase in Mean Squared Error).\

```{r, message = FALSE, warning = FALSE}
predictors_data = data %>%
  select(Education, Income, Recency, Complain, Response, Enrollment, 
         Age, Living_Status, Family_Size, Accepted)

data_clean_purchases = data.frame(predictors_data, 
                                  Total_Purchases = data$Total_Purchases)

rf_purchases = randomForest(Total_Purchases ~ ., data = data_clean_purchases, importance = TRUE)

data_clean_spending = data.frame(predictors_data, 
                                 Total_Spending = data$Total_Spending)

rf_spending = randomForest(Total_Spending ~ ., data = data_clean_spending, importance = TRUE)

# View importance
imp_purchases = importance(rf_purchases)
imp_purchases = imp_purchases[order(imp_purchases[, "%IncMSE"], decreasing = TRUE), ]

imp_spending = importance(rf_spending)
imp_spending = imp_spending[order(imp_spending[, "%IncMSE"], decreasing = TRUE), ]

imp_pur_df = data.frame(Variable = rownames(imp_purchases), IncMSE = imp_purchases[, "%IncMSE"])
imp_spend_df = data.frame(Variable = rownames(imp_spending), IncMSE = imp_spending[, "%IncMSE"])

imp_pur_df = imp_pur_df[order(imp_pur_df$IncMSE, decreasing = TRUE), ][1:3, ]
imp_spend_df = imp_spend_df[order(imp_spend_df$IncMSE, decreasing = TRUE), ][1:3, ]

p1 = ggplot(imp_pur_df, aes(x = reorder(Variable, IncMSE), y = IncMSE)) +
  geom_col(fill = "darkblue") + 
  coord_flip() +
  labs(title = "Top Predictors for Purchases", x = "", y = "%IncMSE") +
  theme_minimal()

p2 = ggplot(imp_spend_df, aes(x = reorder(Variable, IncMSE), y = IncMSE)) +
  geom_col(fill = "darkgreen") +
  coord_flip() +
  labs(title = "Top Predictors for Spending", x = "", y = "%IncMSE") +
  theme_minimal()

grid.arrange(p1, p2, ncol = 2)
```

Top four variables sorted by %IncMSE for customer spending and purchases are: `Income`, `Enrollment`, `Accepted`, and `Family_Size`. Therefore, we can assume that these three predictors are the ones that affect customer purchasing behavior the most. In other words, an individual's income, how long the individual has enrolled with the company, their acceptance rate to offers, and their family size would largely determine one's purchasing and spending behaviors. Moreover, `Income` seems to dominate both customer spending and purchase due to its significantly high percent increase in Mean Squared Error (over 150) when comparing to the other features (they all have %IncMSE less than 75). \


Next, we can visualize the relationships between the numeric variables throughout a heatmap, which can help validate our findings from Random Forest. \

```{r, fig.width=5.5, fig.height=5.5, warning=FALSE, message=FALSE}
numeric_data = data %>% select(Income, Recency, Complain, 
                               Response, Enrollment, Age, Total_Spending, 
                               Total_Purchases, Family_Size, Accepted)

cor_matrix = cor(numeric_data, use = "complete.obs")

# Plot heatmap
corrplot(cor_matrix, 
         method = "color", 
         type = "upper", 
         tl.col = "black", 
         addCoef.col = "black",
         title = "Correlation Heatmap of Numetic Variables",
         mar = c(0, 0, 2, 0))        

```

From the heatmap, we can see that `Income` is highly correlated with `Total_Spending` (r=0.67) and `Total_Purchases` (r=0.57). This confirms our key observation from Random Forests, which declares income as the dominant feature for total purchases and spending. Moreover, it is worth mentioning that `Family_Size` is negatively correlated with `Total_Spending` (-0.42) and `Total_Purchases` (-0.20), indicating that the larger the family size, the less the spending and purchases, which might seem like a slightly counter-intuitive result. \



**Linear Regression**

Now, we move on to see how much each purchase channel contributes to total spending and total purchases. We will fit a linear model to quantify channel-specific contributions to spending. \

```{r}
# linear regression model looking at how much each channel contributes to spending
channel_spending_model = lm(Total_Spending ~ NumWebPurchases + NumCatalogPurchases + NumStorePurchases,
                            data = data)

channel_spending_summary = summary(channel_spending_model)

kable(channel_spending_summary$coefficients, 
      caption = "Linear Model: Total Spending", 
      digits = 3) 

```

Predictor `NumCatalogPurchases` has the largest slope (116.088), indicating that customers tend to spend more money while shopping using a catalog. Now we use visualization to look at the distribution of purchases through different channels. \ 

```{r}
# Purchases
channel_purchases = data.frame(
  Channel = c("Web", "Catalog", "Store"),
  Purchases = c(sum(data$NumWebPurchases, na.rm = TRUE),
               sum(data$NumCatalogPurchases, na.rm = TRUE),
               sum(data$NumStorePurchases, na.rm = TRUE))
)

ggplot(channel_purchases, aes(x = Channel, y = Purchases, fill = Channel)) +
  geom_col() +
  geom_text(aes(label = round(Purchases, 0)), vjust = -0.5, size = 4) +
  labs(title = "Total Purchase Through Each Channel",
       y = "Total Purchase") +
  scale_fill_manual(values = c("steelblue", "palegreen3", "salmon")) +
  theme_minimal()
```

Interestingly, looking at this bar plot, purchases made at store seem to account for the most purchases. Although catalog purchases have the highest spending per transaction, store purchases dominate total transaction volume. \ 



**Clustering**

Next, we move on to exploring whether customers can be grouped into distinct segments based on their personalities and behaviors. We will be start with Model-Based Clustering with Gaussian Mixture Model (GMM), which is a probabilistic model that accommodates clusters of varying shapes and sizes. \ 

```{r}
set.seed(123)

# Gaussian Mixture, using data that has categorical variables turned into numeric ones
gmm_model = Mclust(scaled_data, G = 2:5)
predictors_data$GMMCluster = factor(gmm_model$classification)

# sil_kmeans = silhouette(as.numeric(predictors_data$KMeansCluster), dist(scaled_data))
sil_gmm = silhouette(as.numeric(predictors_data$GMMCluster), dist(scaled_data))

# sil_kmeans_mean = mean(sil_kmeans[, 3])
sil_gmm_mean = mean(sil_gmm[, 3])

clustering_table = data.frame(
  Method = "GMM",
  `Silhouette Coefficient` = round(sil_gmm_mean, 3)
)

kable(clustering_table, 
      caption = "Silhouette Coefficients for Model-Based Clustering", 
      digits = 3)
```

We can see from the table that Gaussian Mixture clustering gives us a very weak Silhouette Coefficients of 0.063, indicating bad clustering. This result suggests that clustering might not be appropriate for this dataset, and customers cannot be grouped into distinct segments based on their personalities and behaviors. Instead, customer traits are better understood as continuous variables rather than discrete segments. \ 

Next, we try Hierarchical Clustering with Gower Distance to see if we get similar results. Using Gower distance, we will be able to handles mixed data types. \

```{r dendrogram-plot, fig.align='center', fig.width=12, fig.height=6, warning=FALSE, message=FALSE}
clustering_data = data %>%
  select(
    Income, Recency, Complain, Age, Enrollment, Total_Spending, 
    Total_Purchases, Family_Size, Education, Living_Status, Accepted, Response
  ) %>%
  mutate(across(c(Education, Living_Status), as.factor))

gower_dist = daisy(clustering_data, metric = "gower")

hclust_result = hclust(as.dist(gower_dist), method = "ward.D2")

fviz_dend(
  hclust_result,
  k = 3,
  cex = 0.6,
  main = "Dendrogram of Customer Data",
  xlab = "Observations",
  ylab = "Gower Distance",
  horiz = FALSE  
) 
```

```{r}
data$Cluster = cutree(hclust_result, k = 3)

sil_score = silhouette(data$Cluster, as.dist(gower_dist))  # <<<<<<<<<< fixed here

clustering_table = data.frame(
  Method = "Hierarchical Clustering",
  Silhouette_Score = round(mean(sil_score[, "sil_width"]), 3),
  Number_of_Clusters = 3
)

kable(
  clustering_table,
  caption = "Clustering Performance Metrics",
  col.names = c("Method", "Silhouette Coefficient", "Number of Clusters"),
  digits = 3
)

```

Using Hierarchical Clustering, we got a Silhouette Scores of 0.299, which is much better than the score we got from using Gaussian Mixture, although still being unsatisfactorily low. Let's explore the traits of each cluster. \

```{r}
data$Cluster = cutree(hclust_result, k = 3)

cluster_summary = data %>%
     group_by(Cluster) %>%
     summarise(
         Avg_Income = mean(Income),
         Avg_Spending = mean(Total_Spending),
         Avg_Enrollment = mean(Enrollment),
         Catalog_Purchases = mean(NumCatalogPurchases),
         N = n()
     )

kable(cluster_summary, caption = "Cluster Profiles")
```

Hierarchical clustering gives us 3 clusters. We can see from this printed summary that cluster 1 has a higher average income, average spending, and catalog purchases than clusters 2 and 3. The three clusters have similar average enrollment time, while cluster 3 has the largest population. 

# Discussion

PCA revealed high intrinsic dimensionality in the data, requiring 9 principal components to explain at least 70% of the variance. The absence of an ideal "elbow" in the scree plot suggests no dominant linear relationships, limiting the utility of PCA. Random Forest identified income as the strongest predictor of both total purchases (%IncMSE over 150) and spending (%IncMSE over 150). The heatmap confirmed this finding by showing a strong correlation between income and total spending (r = 0.72) and between income and total purchase (r = 0.68). Other strong predictors pointed out by Random Forest include enrollment lengths, offer acceptance rate, and family size. Moreover, among the three purchase channels, catalog purchases yield the highest spending, possibly due to product offerings, while store purchases contribute to the majority of transactions, highlighting high customer demand for in-store purchases and the need for in-store promotions. Lastly, the low Silhouette scores we got from conducting Gaussian Mixture Model clustering (Silhouette Score = 0.063) indicate no meaningful customer segments. However, Hierarchical Clustering with Gower Distance did a better job at clustering (Silhouette Score = 0.299). This difference might be introduced due to the fact that Gower distance accommodates mixed data types. Nevertheless, both clustering methods indicate limitations in segmenting customers, directing against discrete grouping in favor of continuous trait analysis. \ 


# Conclusion
This project analyzed 2,240 customer records to identify key drivers of purchasing behavior and campaign engagement. It highlights three insights: 

First, four traits dominate purchasing behavior: income, enrollment duration, offer acceptance, and family size are dominant predictors of customer behavior. Therefore, marketing strategies can consider prioritizing personalized campaigns based on these variables.

Second, catalog purchases maximize revenue, while store purchases drive purchase frequency. Different strategies can be used to allocate resources to catalog promotions for high-income customers and in-store promotions for broader audiences.

Lastly, segmentation of customers is limited. Both clustering methods produced relatively weak groups by giving low Silhouette Scores.  \ 

```{r, results='asis', echo=FALSE}
cat("\\newpage")
```

# Appendix
```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```





